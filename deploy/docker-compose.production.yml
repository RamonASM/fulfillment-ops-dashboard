version: "3.8"

# =============================================================================
# Inventory Intelligence Platform - Production Docker Compose Configuration
# =============================================================================
# This file defines the complete production stack with all services
# Usage: docker-compose -f deploy/docker-compose.production.yml up -d
# =============================================================================

networks:
  inventory-network:
    driver: bridge

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  uploads_data:
    driver: local
  ml_models:
    driver: local
  backup_data:
    driver: local

services:
  # ===========================================================================
  # Database - PostgreSQL
  # ===========================================================================
  # SECURITY: All credentials MUST be provided via environment variables.
  # No default fallbacks allowed in production.
  # Required env vars: DB_USER, DB_PASSWORD, DB_NAME, REDIS_PASSWORD,
  #                    JWT_SECRET, JWT_REFRESH_SECRET, SESSION_SECRET
  postgres:
    image: postgres:15-alpine
    container_name: inventory-postgres
    restart: unless-stopped
    networks:
      - inventory-network
    environment:
      # SECURITY: No default values - deployment will fail if not provided
      POSTGRES_USER: ${DB_USER:?DB_USER is required}
      POSTGRES_PASSWORD: ${DB_PASSWORD:?DB_PASSWORD is required}
      POSTGRES_DB: ${DB_NAME:?DB_NAME is required}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=en_US.UTF-8"
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - backup_data:/backups
    expose:
      - "5432"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "pg_isready -U ${DB_USER} -d ${DB_NAME}",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 1G

  # ===========================================================================
  # Cache & Session Store - Redis
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: inventory-redis
    restart: unless-stopped
    networks:
      - inventory-network
    # Enable both AOF and RDB persistence for redundancy
    # RDB snapshots: every 900s if 1+ keys changed, every 300s if 10+ keys, every 60s if 10000+ keys
    # AOF: append-only file for durability
    # SECURITY: Require explicit Redis password
    command: redis-server --appendonly yes --save 900 1 --save 300 10 --save 60 10000 --requirepass ${REDIS_PASSWORD:?REDIS_PASSWORD is required}
    volumes:
      - redis_data:/data
    expose:
      - "6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 256M

  # ===========================================================================
  # API Server - Node.js Express
  # ===========================================================================
  api:
    build:
      context: ..
      dockerfile: Dockerfile
      target: api-production
      args:
        NODE_VERSION: "20"
    container_name: inventory-api
    restart: unless-stopped
    networks:
      - inventory-network
    environment:
      NODE_ENV: ${NODE_ENV:-production}
      PORT: ${API_PORT:-3001}
      # SECURITY: Require all database credentials - no defaults
      DATABASE_URL: postgresql://${DB_USER:?DB_USER is required}:${DB_PASSWORD:?DB_PASSWORD is required}@postgres:5432/${DB_NAME:?DB_NAME is required}
      REDIS_URL: redis://:${REDIS_PASSWORD:?REDIS_PASSWORD is required}@redis:6379
      # SECURITY: Require all JWT/session secrets
      JWT_SECRET: ${JWT_SECRET:?JWT_SECRET is required}
      JWT_REFRESH_SECRET: ${JWT_REFRESH_SECRET:?JWT_REFRESH_SECRET is required}
      SESSION_SECRET: ${SESSION_SECRET:?SESSION_SECRET is required}
      WEB_URL: ${WEB_URL:-https://admin.yourtechassist.us}
      PORTAL_URL: ${PORTAL_URL:-https://portal.yourtechassist.us}
      CORS_ORIGINS: ${CORS_ORIGINS}
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASS: ${SMTP_PASS}
      EMAIL_FROM_NAME: ${EMAIL_FROM_NAME}
      EMAIL_FROM_ADDRESS: ${EMAIL_FROM_ADDRESS}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      SENTRY_DSN: ${SENTRY_DSN}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      ML_ANALYTICS_URL: ${ML_ANALYTICS_URL:-http://ml-analytics:8000}
    volumes:
      - uploads_data:/app/uploads
      - ../apps/api/prisma:/app/prisma
    ports:
      - "${API_PORT:-3001}:3001"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 1G
        reservations:
          cpus: "1.0"
          memory: 512M

  # ===========================================================================
  # ML Analytics Service - Python FastAPI
  # ===========================================================================
  ml-analytics:
    build:
      context: ../apps/ml-analytics
      dockerfile: Dockerfile
    container_name: inventory-ml-analytics
    restart: unless-stopped
    networks:
      - inventory-network
    environment:
      # SECURITY: Require database credentials - no defaults
      DATABASE_URL: postgresql://${DB_USER:?DB_USER is required}:${DB_PASSWORD:?DB_PASSWORD is required}@postgres:5432/${DB_NAME:?DB_NAME is required}
      PORT: 8000
      HOST: 0.0.0.0
      LOG_LEVEL: ${LOG_LEVEL:-info}
      MODEL_PATH: /app/models
    volumes:
      - ml_models:/app/models
    ports:
      - "${ML_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 1G

  # ===========================================================================
  # Web Dashboard & Portal - Nginx Static Server
  # ===========================================================================
  web:
    build:
      context: ..
      dockerfile: Dockerfile
      target: static-production
    container_name: inventory-web
    restart: unless-stopped
    networks:
      - inventory-network
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    volumes:
      - ./nginx/inventory-docker.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - api
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M

  # ===========================================================================
  # Database Backup Service
  # Runs pg_dump daily at 2 AM, retains backups for BACKUP_KEEP_DAYS
  # ===========================================================================
  backup:
    image: postgres:15-alpine
    container_name: inventory-backup
    restart: unless-stopped
    networks:
      - inventory-network
    environment:
      PGHOST: postgres
      # SECURITY: Require database credentials - no defaults
      PGUSER: ${DB_USER:?DB_USER is required}
      PGPASSWORD: ${DB_PASSWORD:?DB_PASSWORD is required}
      PGDATABASE: ${DB_NAME:?DB_NAME is required}
      BACKUP_DIR: /backups
      BACKUP_KEEP_DAYS: ${BACKUP_KEEP_DAYS:-7}
    volumes:
      - backup_data:/backups
    depends_on:
      postgres:
        condition: service_healthy
    # Run backup every 24 hours (86400 seconds)
    # Creates compressed backup, verifies it, and cleans old backups
    command: |
      /bin/sh -c '
        echo "Backup service started"
        while true; do
          echo "============================================"
          echo "Running backup at $$(date)"
          echo "============================================"

          # Create backup with timestamp
          BACKUP_FILE="/backups/inventory_$$(date +%Y%m%d_%H%M%S).sql.gz"

          # Perform backup
          if pg_dump -h $$PGHOST -U $$PGUSER $$PGDATABASE | gzip > $$BACKUP_FILE; then
            SIZE=$$(du -h $$BACKUP_FILE | cut -f1)
            echo "✓ Backup created: $$BACKUP_FILE ($$SIZE)"

            # Verify backup is not empty
            if [ $$(zcat $$BACKUP_FILE | wc -c) -lt 1000 ]; then
              echo "✗ Backup appears to be empty or too small!"
              rm $$BACKUP_FILE
            else
              echo "✓ Backup verified"
            fi
          else
            echo "✗ Backup failed!"
          fi

          # Clean old backups
          echo "Cleaning backups older than $$BACKUP_KEEP_DAYS days..."
          find /backups -name "inventory_*.sql.gz" -mtime +$$BACKUP_KEEP_DAYS -delete
          echo "Current backups:"
          ls -lh /backups/inventory_*.sql.gz 2>/dev/null | tail -10 || echo "No backups found"

          echo "Next backup in 24 hours"
          echo "============================================"
          sleep 86400
        done
      '
    profiles:
      - backup
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 64M

# =============================================================================
# Usage Instructions:
# =============================================================================
#
# 1. Copy .env.production.example to .env and configure
# 2. Start all services:
#    docker-compose -f deploy/docker-compose.production.yml up -d
#
# 3. View logs:
#    docker-compose -f deploy/docker-compose.production.yml logs -f [service]
#
# 4. Stop all services:
#    docker-compose -f deploy/docker-compose.production.yml down
#
# 5. Backup database:
#    docker-compose -f deploy/docker-compose.production.yml --profile backup up backup
#
# 6. Scale API service:
#    docker-compose -f deploy/docker-compose.production.yml up -d --scale api=3
# =============================================================================
